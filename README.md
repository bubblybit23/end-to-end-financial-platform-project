# Financial Data Pipeline Project

## Overview

This project automates the generation, cleaning, and reconciliation of financial transaction data, simulating a system similar to Grab. The pipeline is designed to:

1.  **Generate synthetic data:** Create realistic-looking transaction data, including accounts, Grab transactions, and partner transactions, with controlled discrepancies.
2.  **Clean the data:** Process the raw data to ensure consistency and accuracy, handling various date formats, data type conversions, and string cleaning.
3.  **Reconcile the data:** Compare Grab transactions with partner transactions to identify matches, discrepancies, and missing records.
4.  **Store the data:** Save the generated, cleaned, and reconciled data to both CSV files and a PostgreSQL database. The CSV files are stored in dated subfolders within the `scripts_output` directory.
5.  **Automate the process:** Schedule the entire pipeline to run daily.
6.  **Error handling:** Implement robust logging and email notifications for failures.

## Project Structure

The project is organized as follows:

end-to-end-financial-platform-project/
├── .env                    # Stores sensitive information (database credentials, email settings)
├── automation_config.yaml  # Configuration for the automation schedule and scripts
├── automation.log          # Main automation log
├── reconciliation.log      # Log for the reconciliation process
├── src/                    # Source code directory
│   ├── data_cleaning/      # Data cleaning scripts
│   │   └── clean_data.py   # Cleans raw data
│   ├── data_generation/    # Data generation scripts
│   │   └── generate_data_daily.py # Generates synthetic data for a single day
│   ├── data_reconciliation/ # Data reconciliation scripts
│   │   └── reconcile_data_sql.py # Reconciles Grab and partner transactions using SQL
├── scripts_output/         # Directory where script outputs (CSV files) are saved, organized by date.
├── .gitignore              # Specifies intentionally untracked files that Git should ignore
├── README.md               # Project documentation
└── requirements.txt          # Lists the project's dependencies

**Key Points:**

* The `.env`, `automation_config.yaml`, `.gitignore`, `requirements.txt`, and `run_automation.py` files, and the `logs/` directory are located in the project root directory.
* The source code for the data processing scripts is located in the `src/` directory.
* The `scripts_output/` directory in the project root will contain subdirectories named with the target date (e.g., `20240724`) to store the generated and cleaned CSV files.

## Scripts

### 1. `src/data_generation/generate_data_daily.py`

* **Purpose:** Generates synthetic financial transaction data for a single day.
* **Key Features:**
    * Generates data for accounts, Grab transactions, and partner transactions.
    * Introduces controlled discrepancies (missing, extra, mismatched amounts/statuses) to simulate real-world data.
    * Timestamps are generated in the Philippine timezone (UTC+8).
    * Uses `faker` for realistic data and `uuid4` for unique IDs.
    * Saves generated data to CSV files in a dated subfolder within the `scripts_output` directory (e.g., `scripts_output/20240724/`).
    * Loads data into a PostgreSQL database.
* **Configuration:**
    * Discrepancy rates and the number of generated records are configurable within the script.
    * Database connection details are read from a `.env` file in the project root.
* **Output:**
    * CSV files: `accounts_{YYYYMMDD}.csv`, `grab_transactions_{YYYYMMDD}.csv`, `partner_transactions_{YYYYMMDD}.csv` in `scripts_output/{YYYYMMDD}/`
* **Tables Created:**
    * `accounts_{YYYYMMDD}`
    * `grab_transactions_{YYYYMMDD}`
    * `partner_transactions_{YYYYMMDD}`

### 2. `src/data_cleaning/clean_data.py`

* **Purpose:** Cleans the raw data generated by `generate_data_daily.py`.
* **Key Features:**
    * Reads raw data from CSV files (from a dated subfolder within `scripts_output`).
    * Performs data cleaning operations:
        * Datetime parsing with multiple formats.
        * Data type conversions.
        * String normalization (stripping, lowercasing, uppercasing).
    * Saves cleaned data to new CSV files in a dated subfolder within the `scripts_output` directory.
    * Uses staging tables in PostgreSQL for efficient data loading.
* **Dependencies:** `pandas`, `SQLAlchemy`
* **Output:**
    * CSV files: `cleaned_accounts_{YYYYMMDD}.csv`, `cleaned_grab_transactions_{YYYYMMDD}.csv`, `cleaned_partner_transactions_{YYYYMMDD}.csv` in `scripts_output/{YYYYMMDD}/`
* **Tables Created:**
    * `cleaned_accounts_{YYYYMMDD}`
    * `cleaned_grab_transactions_{YYYYMMDD}`
    * `cleaned_partner_transactions_{YYYYMMDD}`

### 3. `src/data_reconciliation/reconcile_data_sql.py`

* **Purpose:** Reconciles cleaned Grab transactions with cleaned partner transactions.
* **Key Features:**
    * Reads cleaned data from CSV files (from a dated subfolder within `scripts_output`).
    * Uses `pandasql` to execute SQL queries on Pandas DataFrames for reconciliation:
        * Identifies matching transactions.
        * Identifies Grab-only and partner-only transactions.
        * Identifies discrepancies in amount, status, and currency code.
    * Saves reconciliation results to CSV files in a dated subfolder within the `scripts_output` directory.
    * Loads reconciliation results into PostgreSQL tables.
* **Dependencies:** `pandas`, `pandasql`, `psycopg2`
* **Output:**
    * CSV files: `reconciled_transactions_{YYYYMMDD}.csv`, `reconciled_grab_only_{YYYYMMDD}.csv`, `reconciled_partner_only_{YYYYMMDD}.csv`, and `reconciled_discrepant_{YYYYMMDD}.csv` in `scripts_output/{YYYYMMDD}/`
* **Tables Created:**
    * `reconciled_transactions_{YYYYMMDD}`
    * `reconciled_grab_only_{YYYYMMDD}`
    * `reconciled_partner_only_{YYYYMMDD}`
    * `reconciled_discrepant_{YYYYMMDD}`

### 4. `src/run_automation.py`

* **Purpose:** Orchestrates the execution of the data pipeline scripts.
* **Key Features:**
    * Loads script execution order and schedule from `automation_config.yaml` in the project root.
    * Runs the scripts in the specified order with delays.
    * Handles daily scheduling.
    * Implements logging.
    * Sends email notifications on script failures.
* **Configuration:**
    * Reads script paths, execution order, and schedule from `automation_config.yaml` in the project root.
    * Email settings (SMTP server, credentials, sender/receiver) are read from a `.env` file in the project root.
* **Error Handling:** If a script fails, the subsequent scripts are skipped, and an email notification is sent.

### 5. `automation_config.yaml`

* **Purpose**: Configuration file for the `run_automation.py` script.
* **Content**:
    * `schedule`: Defines the frequency and time of the automation.
    * `scripts`: A list of scripts to execute, in order, along with a delay (in seconds) to wait after each script finishes.  The paths to the scripts *must* be absolute paths.

### 6. `.env`

* **Purpose**: Stores sensitive information, such as database credentials and email server settings. This file is located in the project root.
* **Content**:

    ```text
    DB_HOST=your_db_host
    DB_NAME=your_db_name
    DB_USER=your_db_user
    DB_PASSWORD=your_db_password
    DB_PORT=your_db_port
    SMTP_SERVER=your_smtp_server
    SMTP_PORT=your_smtp_port # e.g., 587
    SMTP_USERNAME=your_smtp_username
    SMTP_PASSWORD=your_smtp_password
    SENDER_EMAIL=your_sender_email
    RECEIVER_EMAIL=your_receiver_email
    LOG_FILE=automation.log # Name of the log file
    ```

    **Note:** *Do not* commit your `.env` file to a public repository.

## Prerequisites

1.  **Python 3.8 or higher:** Ensure you have Python installed.
2.  **PostgreSQL:** You need a running PostgreSQL database.
3.  **Virtual Environment (Recommended):** It's highly recommended to use a virtual environment to manage project dependencies.
4.  **`.env` File:** Create a `.env` file in the project root directory and fill in the database and email settings.
5.  **`automation_config.yaml`**: Create this configuration file in the project root. A sample is provided.

## Installation

1.  **Clone the repository:**

    ```bash
    git clone <your_repository_url>
    cd end-to-end-financial-platform-project
    ```
2.  **Create a virtual environment (recommended):**

    ```bash
    python -m venv .venv
    ```

    * Activate the virtual environment:
        * On Windows: `.\.venv\Scripts\activate`
        * On macOS/Linux: `source .venv/bin/activate`
3.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```
4.  **Set up your `.env` file:** Create a `.env` file in the project root and fill in the necessary values (database credentials, email settings).
5.  **Set up your `automation_config.yaml` file:** Create this file in the project root based on the sample provided. Ensure that the script paths in this file are absolute paths and are correct for your system.

## Configuration

### 1. Database Configuration

* Create a PostgreSQL database.
* In the `.env` file (in the project root), set the following variables:
    * `DB_HOST`
    * `DB_NAME`
    * `DB_USER`
    * `DB_PASSWORD`
    * `DB_PORT`

### 2. Email Configuration (Optional)

* If you want to receive email notifications on script failures, configure the following variables in the `.env` file (in the project root):
    * `SMTP_SERVER`
    * `SMTP_PORT`
    * `SMTP_USERNAME`
    * `SMTP_PASSWORD`
    * `SENDER_EMAIL`
    * `RECEIVER_EMAIL`

### 3. Automation Schedule

* The `automation_config.yaml` file (in the project root) controls the script execution schedule.
    * `frequency`: Currently only "daily" is supported.
    * `time`: The time of day to run the scripts (HH:MM format).
    * `scripts`: A list of scripts to run, in order.
        * `name`: A descriptive name for the script.
        * `path`: The **absolute** path to the script. **You must update these paths in `automation_config.yaml` to match your system.**
        * `delay_after`: The number of seconds to wait after the script finishes before starting the next one.

## Running the Automation

The pipeline is designed to be run automatically. Here's how to set up automation on different platforms:

### 1. Using Windows Task Scheduler

1.  Open Task Scheduler.
2.  Create a new basic task.
3.  **Trigger:** Set the schedule (e.g., daily).
4.  **Action:**
    * Start a program.
    * Program/script: `C:\Windows\System32\cmd.exe` (or the path to your `cmd.exe`)
    * Add arguments: `/c "<path_to_your_python_executable> <path_to_run_automation.py>"`
        * Example: `/c "C:\Users\youruser\AppData\Local\Programs\Python\Python39\python.exe C:\path\to\your\end-to-end-financial-platform-project\run_automation.py"`
        * Make sure to use the full path to your Python executable. You can find this by typing `where python` in a command prompt in your activated virtual environment.
        * The `/c` argument tells `cmd.exe` to execute the command and then terminate.
5.  **Start in (Optional):** Set the "Start in" directory to the project root (the directory containing `run_automation.py`).
6.  Finish the wizard. You may need to adjust security settings.

### 2. Using macOS Launchd

1.  Create a property list file (e.g., `com.example.financial_pipeline.plist`) in `~/Library/LaunchAgents/` or `/Library/LaunchDaemons/` (for system-wide). Use `sudo` for the latter.
2.  Edit the plist file with a text editor:

    ```xml
    <?xml version="1.0" encoding="UTF-8"?>
    <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "[http://www.apple.com/DTDs/PropertyList-1.0.dtd](http://www.apple.com/DTDs/PropertyList-1.0.dtd)">
    <plist version="1.0">
    <dict>
        <key>Label</key>
        <string>com.example.financial_pipeline</string> <# Change this to a unique identifier #>
        <key>Program</key>
        <string>/Users/youruser/.venv/bin/python</string> <# *Use the full path to your python executable* #>
        <key>ProgramArguments</key>
        <array>
            <string>/Users/youruser/end-to-end-financial-platform-project/run_automation.py</string> <# *Full path to run_automation.py* #>
        </array>
        <key>StartCalendarInterval</key>
        <dict>
            <key>Hour</key>
            <integer>22</integer>  <# Set the hour #>
            <key>Minute</key>
            <integer>33</integer> <# Set the minute #>
        </dict>
        <key>StandardOutPath</key>
        <string>/tmp/financial_pipeline.out.log</string> <# Change if desired #>
        <key>StandardErrorPath</key>
        <string>/tmp/financial_pipeline.err.log</string> <# Change if desired #>
    </dict>
    </plist>
    ```

    * **Important:** Replace the paths with the actual paths on your system. Use the full path to the Python executable within your virtual environment.
    * Change the `Label` to something unique.
    * Adjust the `Hour` and `Minute` to match your desired run time.
3.  Load the launch agent:

    ```bash
    launchctl load ~/Library/LaunchAgents/com.example.financial_pipeline.plist
    ```

    or, for system-wide:

    ```bash
    sudo launchctl load /Library/LaunchDaemons/com.example.financial_pipeline.plist
    ```
4.  Start the launch agent:

    ```bash
    launchctl start com.example.financial_pipeline
    ```

### 3. Using Linux Cron

1.  Open the crontab file:

    ```bash
    crontab -e
    ```

    * You might be asked to choose an editor (e.g., nano, vim).
2.  Add a line to schedule the script:

    ```cron
    33 22 * * * /home/youruser/.venv/bin/python /home/youruser/end-to-end-financial-platform-project/run_automation.py
    ```

    * **Important:** Replace the paths with the actual paths on your system. Use the full path to the Python executable in your virtual environment.
    * The cron syntax is: `minute hour day month weekday command`
    * In this example, the script will run at 22:33 every day.
    * To understand cron syntax, you can use online tools like [crontab guru](https://crontab.guru/).
3.  Save the crontab file.

## Logging

The scripts use the `logging` module to record events, errors, and information. Logs are written to the `logs/` directory in the project root:

* `automation.log`: Main log for the `run_automation.py` script.
* `reconciliation.log`: Log for the `reconcile_data_sql.py` script.

## Error Handling

* The `run_automation.py` script captures exceptions during script execution.
* If a script fails:
    * The error is logged.
    * An email notification is sent (if email is configured in `.env`).
    * The remaining scripts in the pipeline are skipped.
* Each script also has its own error handling (e.g., database connection errors, file errors).

## Scalability and Future Improvements

* **Scalability:**
    * For larger datasets, consider using a more robust data processing framework like Apache Spark or Apache Kafka.
    * Optimize database operations (e.g., using batch inserts, indexing).
    * Implement parallel processing where applicable.
* **Future Improvements:**
    * Implement more sophisticated data validation and cleaning rules.
    * Add more comprehensive unit and integration tests.
    * Create a web interface or API to trigger and monitor the pipeline.
    * Implement data quality monitoring.
    * Containerize the application using Docker for easier deployment.
    * Add support for other data sources and formats.
    * Implement real-time data processing.
    * Improve the email notification system.
    * Add a retry mechanism for failed scripts.
